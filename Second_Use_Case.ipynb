{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 1:  Video Upload and Frame Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Directory for video uplaod and frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame extraction completed. Frames are saved in Video2_frames\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to create directory for storing frames\n",
    "def create_frame_dir(video_file):\n",
    "    frame_dir = os.path.splitext(video_file)[0] + \"_frames\"\n",
    "    if not os.path.exists(frame_dir):\n",
    "        os.makedirs(frame_dir)\n",
    "    return frame_dir\n",
    "\n",
    "# Function to extract and save frames from the video\n",
    "def extract_frames(video_file):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    # Create directory to store the frames\n",
    "    frame_dir = create_frame_dir(video_file)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # If no frame is captured, exit\n",
    "        \n",
    "        # Save frame as image\n",
    "        frame_path = os.path.join(frame_dir, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Extracted frame {frame_count}\")\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Frame extraction completed. Frames are saved in {frame_dir}\")\n",
    "\n",
    "# Call the function with your video file path\n",
    "video_file = \"Video2.mp4\"  # Replace with your video file\n",
    "extract_frames(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Player Detection Using YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use YOLOv5 to detect players in frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dhira/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-6 Python-3.12.3 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load YOLOv5 pre-trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Detect players in the extracted frames\n",
    "def detect_players_in_frames(frame_dir):\n",
    "    frame_paths = list(Path(frame_dir).glob('*.jpg'))\n",
    "    \n",
    "    for frame_path in frame_paths:\n",
    "        # Perform detection\n",
    "        results = model(frame_path)\n",
    "        \n",
    "        # Display results (bounding boxes around detected players)\n",
    "        # results.show()  # This will show the image with detection\n",
    "\n",
    "        # If you want to save the results, uncomment the following line\n",
    "        # results.save()  # This will save images with bounding boxes to the current directory\n",
    "        \n",
    "        # print(f\"Detected players in {frame_path}\")\n",
    "\n",
    "# Call the function with the frame directory\n",
    "frame_dir = os.path.splitext(video_file)[0] + \"_frames\"  # Replace with your frame directory\n",
    "detect_players_in_frames(frame_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tracking the Player Across Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "scikit-learn version: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Player tracking with DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dhira/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-6 Python-3.12.3 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the YOLOv5 model (Ensure that it's already set up)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Assuming the DeepSORT object is replaced with a custom tracker or placeholder\n",
    "class SimpleTracker:\n",
    "    def __init__(self):\n",
    "        self.next_id = 0\n",
    "        self.objects = {}\n",
    "\n",
    "    def update(self, bboxes):\n",
    "        new_objects = {}\n",
    "        for bbox in bboxes:\n",
    "            new_objects[self.next_id] = bbox\n",
    "            self.next_id += 1\n",
    "        self.objects = new_objects\n",
    "        return [[*bbox, obj_id] for obj_id, bbox in self.objects.items()]\n",
    "\n",
    "# Initialize a simple tracker\n",
    "tracker = SimpleTracker()\n",
    "\n",
    "# Function to track players across frames\n",
    "def track_players_in_video(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit if no frame is captured\n",
    "\n",
    "        # Perform detection (similar to part 2)\n",
    "        results = model(frame)\n",
    "        bboxes = results.xyxy[0][:, :4].cpu().numpy()  # Bounding boxes\n",
    "        confs = results.xyxy[0][:, 4].cpu().numpy()    # Confidence scores\n",
    "\n",
    "        # Perform tracking using a simple tracker (replace deepsort)\n",
    "        outputs = tracker.update(bboxes)\n",
    "\n",
    "        # Draw the bounding boxes and track IDs on the frame\n",
    "        for output in outputs:\n",
    "            x1, y1, x2, y2, track_id = output[:5]\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"ID: {track_id}\", (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Tracking\", frame)\n",
    "        frame_count += 1\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the tracking function\n",
    "video_file = \"Video2.mp4\"  # Replace with your video file\n",
    "track_players_in_video(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Pose Estimation Using OpenPose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Implement Pose Estimation on Extracted Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dhira/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-6 Python-3.12.3 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Load YOLOv5 pre-trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Function to perform pose estimation on a single frame\n",
    "def pose_estimation_on_frame(frame):\n",
    "    # Convert frame to RGB as MediaPipe works with RGB images\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform pose estimation\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Draw keypoints on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(annotated_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    return annotated_frame, results.pose_landmarks\n",
    "\n",
    "# Function to detect players and perform pose estimation\n",
    "def process_video(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit if no frame is captured\n",
    "\n",
    "        # Perform detection using YOLOv5\n",
    "        results = model(frame)\n",
    "        bboxes = results.xyxy[0].cpu().numpy()  # Bounding boxes\n",
    "\n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Draw bounding boxes for detected players\n",
    "        for bbox in bboxes:\n",
    "            # Check the number of values in bbox\n",
    "            if len(bbox) == 4:\n",
    "                x1, y1, x2, y2 = bbox  # If only bounding box coordinates are returned\n",
    "                conf = 1.0  # Set default confidence if not available\n",
    "            elif len(bbox) >= 5:\n",
    "                x1, y1, x2, y2, conf = bbox[:5]  # Get the first five values\n",
    "            else:\n",
    "                continue  # Skip if bbox has fewer than 4 values\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "\n",
    "        # Display the annotated frame with pose landmarks and bounding boxes\n",
    "        cv2.imshow(\"Pose Estimation and Player Detection\", annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function with your video file\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_video(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define Actions and Recognition Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to calculate the angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)  # First\n",
    "    b = np.array(b)  # Mid\n",
    "    c = np.array(c)  # End\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "    return angle\n",
    "\n",
    "# Function to perform pose estimation on a single frame\n",
    "def pose_estimation_on_frame(frame):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(annotated_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    return annotated_frame, results.pose_landmarks\n",
    "\n",
    "# Action recognition function\n",
    "def recognize_action(landmarks):\n",
    "    if landmarks is not None:\n",
    "        # Define keypoints\n",
    "        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        left_wrist = landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "        \n",
    "        right_shoulder = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        right_elbow = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "        right_wrist = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "        left_hip = landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "        left_knee = landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "        left_ankle = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "\n",
    "        right_hip = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "        right_knee = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "        right_ankle = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
    "\n",
    "        # Calculate angles\n",
    "        shoulder_angle_left = calculate_angle(\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (left_elbow.x, left_elbow.y),\n",
    "            (left_wrist.x, left_wrist.y)\n",
    "        )\n",
    "\n",
    "        shoulder_angle_right = calculate_angle(\n",
    "            (right_shoulder.x, right_shoulder.y),\n",
    "            (right_elbow.x, right_elbow.y),\n",
    "            (right_wrist.x, right_wrist.y)\n",
    "        )\n",
    "\n",
    "        hip_angle_left = calculate_angle(\n",
    "            (left_hip.x, left_hip.y),\n",
    "            (left_knee.x, left_knee.y),\n",
    "            (left_ankle.x, left_ankle.y)\n",
    "        )\n",
    "\n",
    "        hip_angle_right = calculate_angle(\n",
    "            (right_hip.x, right_hip.y),\n",
    "            (right_knee.x, right_knee.y),\n",
    "            (right_ankle.x, right_ankle.y)\n",
    "        )\n",
    "\n",
    "        # Debug outputs for angles\n",
    "        print(f\"Left Shoulder Angle: {shoulder_angle_left}\")\n",
    "        print(f\"Right Shoulder Angle: {shoulder_angle_right}\")\n",
    "        print(f\"Left Hip Angle: {hip_angle_left}\")\n",
    "        print(f\"Right Hip Angle: {hip_angle_right}\")\n",
    "\n",
    "        # Define action recognition based on calculated angles\n",
    "        if shoulder_angle_left < 30 and shoulder_angle_right < 30:  # Shooting\n",
    "            return \"Action: Shooting\"\n",
    "        elif shoulder_angle_left > 150 and shoulder_angle_right > 150:  # Running\n",
    "            return \"Action: Running\"\n",
    "        elif 30 <= shoulder_angle_left <= 150 and 30 <= shoulder_angle_right <= 150:  # Neutral position\n",
    "            return \"Action: Neutral\"\n",
    "        elif hip_angle_left < 30 and hip_angle_right < 30:  # Jumping\n",
    "            return \"Action: Jumping\"\n",
    "        elif hip_angle_left > 160 and hip_angle_right > 160:  # Standing\n",
    "            return \"Action: Standing\"\n",
    "        elif 60 <= hip_angle_left <= 120 and 60 <= hip_angle_right <= 120:  # Bending\n",
    "            return \"Action: Bending\"\n",
    "        elif 45 <= hip_angle_left <= 90 and 45 <= hip_angle_right <= 90:  # Walking\n",
    "            return \"Action: Walking\"\n",
    "        else:\n",
    "            return \"Action: Unknown\"  # Default case\n",
    "\n",
    "    return \"Action: Unknown\"\n",
    "\n",
    "# Process all frames for pose estimation and action recognition\n",
    "def process_pose_estimation_on_video_with_action_recognition(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Debug output for landmarks\n",
    "        if landmarks:\n",
    "            for idx, landmark in enumerate(landmarks.landmark):\n",
    "                print(f\"Landmark {idx}: ({landmark.x}, {landmark.y})\")\n",
    "\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the action recognition function with your video\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_action_recognition(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Generating Feedback for Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Generate Feedback Based on Recognized Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: Generating Feedback for Players\n",
    "\n",
    "def generate_feedback(action):\n",
    "    feedback = \"\"\n",
    "    \n",
    "    # Provide feedback based on the recognized action\n",
    "    if action == \"Action: Shooting\":\n",
    "        feedback = \"Feedback: Focus on improving your shooting technique. Aim for accuracy.\"\n",
    "    elif action == \"Action: Running\":\n",
    "        feedback = \"Feedback: Maintain a steady pace and good form while running.\"\n",
    "    elif action == \"Action: Neutral\":\n",
    "        feedback = \"Feedback: Keep practicing your movements for better fluidity.\"\n",
    "    elif action == \"Action: Jumping\":\n",
    "        feedback = \"Feedback: Focus on explosive power and proper landing technique.\"\n",
    "    elif action == \"Action: Standing\":\n",
    "        feedback = \"Feedback: Maintain a balanced and stable stance. Avoid slouching.\"\n",
    "    elif action == \"Action: Bending\":\n",
    "        feedback = \"Feedback: Ensure your knees are aligned with your toes and your back is straight.\"\n",
    "    elif action == \"Action: Walking\":\n",
    "        feedback = \"Feedback: Practice smooth and controlled strides to enhance your walking efficiency.\"\n",
    "    else:\n",
    "        feedback = \"Feedback: Keep practicing your actions for better performance.\"\n",
    "    \n",
    "    return feedback\n",
    "\n",
    "# Modify the video processing function to include feedback generation\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the feedback generation function with your video\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Simple Implementation of getting video and anlysis them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback for Video2.mp4: Analysis completed for Video2.mp4\n"
     ]
    }
   ],
   "source": [
    "def analyze_video(video_path):\n",
    "    # Placeholder for video analysis logic\n",
    "    # Replace this with actual video analysis code\n",
    "    # For now, let's just return a dummy feedback\n",
    "    return f\"Analysis completed for {video_path}\"\n",
    "\n",
    "def test_videos(video_list):\n",
    "    results = []\n",
    "    for video_path in video_list:\n",
    "        feedback = analyze_video(video_path)  # Call the video analysis function\n",
    "        results.append((video_path, feedback))\n",
    "    return results\n",
    "\n",
    "# Sample usage with your specified video file\n",
    "video_list = ['Video2.mp4']  # Use your specific video file\n",
    "results = test_videos(video_list)\n",
    "for video, feedback in results:\n",
    "    print(f\"Feedback for {video}: {feedback}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_threshold = 0.5  # Initial threshold\n",
    "\n",
    "def set_action_threshold(new_threshold):\n",
    "    global action_threshold\n",
    "    action_threshold = new_threshold\n",
    "\n",
    "# Example of updating the threshold\n",
    "set_action_threshold(0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to log feedback for each frame with timestamp\n",
    "def log_feedback(video_path, action, feedback, frame_number):\n",
    "    with open('feedback_log.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([datetime.now(), video_path, frame_number, action, feedback])\n",
    "\n",
    "# Example of how the function would be used within your video processing loop\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_number = 0  # Initialize frame counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Log the feedback for this frame\n",
    "        log_feedback(video_file, action, feedback, frame_number)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        # Increment frame number\n",
    "        frame_number += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Sample logging within video processing function\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to initialize the database with the appropriate table\n",
    "def initialize_db():\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS performance (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            timestamp TEXT,\n",
    "            video_path TEXT,\n",
    "            frame_number INTEGER,\n",
    "            action TEXT,\n",
    "            feedback TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to insert performance data into the database\n",
    "def insert_performance(video_path, frame_number, action, feedback):\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        INSERT INTO performance (timestamp, video_path, frame_number, action, feedback) \n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    ''', (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), video_path, frame_number, action, feedback))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Initialize the database\n",
    "initialize_db()\n",
    "\n",
    "# Modified video processing function to log actions and feedback frame by frame\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_number = 0  # Initialize frame counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Insert the feedback for this frame into the database\n",
    "        insert_performance(video_file, frame_number, action, feedback)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        # Increment frame number\n",
    "        frame_number += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the video processing function\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import tkinter as tk\n",
    "# from tkinter import ttk, filedialog, messagebox\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Initialize Mediapipe pose detection\n",
    "# mp_pose = mp.solutions.pose\n",
    "# pose = mp_pose.Pose()\n",
    "\n",
    "# # Initialize drawing utilities for Mediapipe\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Calculate angle helper function\n",
    "# def calculate_angle(point1, point2, point3):\n",
    "#     x1, y1 = point1\n",
    "#     x2, y2 = point2\n",
    "#     x3, y3 = point3\n",
    "#     angle = np.degrees(np.arctan2(y3 - y2, x3 - x2) - np.arctan2(y1 - y2, x1 - x2))\n",
    "#     return abs(angle) if angle >= 0 else abs(angle + 360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Recognize Action (based on shoulder angle or other criteria)\n",
    "# def recognize_action(landmarks):\n",
    "#     if landmarks is not None:\n",
    "#         left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "#         left_elbow = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "#         left_wrist = landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "#         shoulder_angle = calculate_angle(\n",
    "#             (left_shoulder.x, left_shoulder.y),\n",
    "#             (left_elbow.x, left_elbow.y),\n",
    "#             (left_wrist.x, left_wrist.y)\n",
    "#         )\n",
    "\n",
    "#         if shoulder_angle < 30:\n",
    "#             action = \"Action: Shooting\"\n",
    "#             feedback = \"Feedback: Improve shooting accuracy.\"\n",
    "#         elif 30 <= shoulder_angle < 45:\n",
    "#             action = \"Action: Dribbling\"\n",
    "#             feedback = \"Feedback: Maintain low center of gravity.\"\n",
    "#         elif 45 <= shoulder_angle < 75:\n",
    "#             action = \"Action: Standing\"\n",
    "#             feedback = \"Maintain balance and posture.\"\n",
    "#         elif 75 <= shoulder_angle < 105:\n",
    "#             action = \"Action: Jumping\"\n",
    "#             feedback = \"Enhance explosive power and landing.\"\n",
    "#         elif 105 <= shoulder_angle < 130:\n",
    "#             action = \"Action: Bending\"\n",
    "#             feedback = \"Align knees and back properly.\"\n",
    "#         elif 130 <= shoulder_angle < 150:\n",
    "#             action = \"Action: Walking\"\n",
    "#             feedback = \"Focus on smooth strides.\"\n",
    "#         elif 150 <= shoulder_angle <= 180:\n",
    "#             action = \"Action: Running\"\n",
    "#             feedback = \"Keep steady pace and form.\"\n",
    "#         else:\n",
    "#             action = \"Action: Unknown\"\n",
    "#             feedback = \"Keep working on form.\"\n",
    "\n",
    "#     else:\n",
    "#         action = \"Action: Unknown\"\n",
    "#         feedback = \"No pose detected.\"\n",
    "\n",
    "#     return action, feedback\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the distance between two points\n",
    "# def calculate_distance(point1, point2):\n",
    "#     return np.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "\n",
    "# # Function for performing pose estimation on each frame\n",
    "# def pose_estimation_on_frame(frame):\n",
    "#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     result = pose.process(frame_rgb)\n",
    "#     return result\n",
    "\n",
    "# # Function to analyze video and track multiple players\n",
    "# def analyze_video(video_path, treeview):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     frame_number = 0\n",
    "#     prev_positions = {}  # To store the previous position of each player\n",
    "#     actions_feedback = []  # To store actions and feedback for later display\n",
    "\n",
    "#     if not cap.isOpened():\n",
    "#         messagebox.showerror(\"Error\", \"Could not open video file.\")\n",
    "#         return\n",
    "\n",
    "#     fps = cap.get(cv2.CAP_PROP_FPS)  # Get the frames per second of the video\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break  # Break if no frame is returned (end of video)\n",
    "\n",
    "#         frame_number += 1\n",
    "#         result = pose_estimation_on_frame(frame)\n",
    "        \n",
    "#         if result.pose_landmarks:\n",
    "#             # If multiple people are detected, result.pose_landmarks will be a list of NormalizedLandmarkList\n",
    "#             if isinstance(result.pose_landmarks, list):\n",
    "#                 for i, landmarks_person in enumerate(result.pose_landmarks):\n",
    "#                     # Calculate center of mass (torso center, e.g., mid-point between shoulders and hips)\n",
    "#                     left_shoulder = landmarks_person.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "#                     right_shoulder = landmarks_person.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "#                     left_hip = landmarks_person.landmark[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "#                     right_hip = landmarks_person.landmark[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "\n",
    "#                     # Calculate the torso center (average of shoulder and hip positions)\n",
    "#                     torso_center_x = (left_shoulder.x + right_shoulder.x + left_hip.x + right_hip.x) / 4\n",
    "#                     torso_center_y = (left_shoulder.y + right_shoulder.y + left_hip.y + right_hip.y) / 4\n",
    "#                     current_position = (torso_center_x, torso_center_y)\n",
    "\n",
    "#                     # If this is not the first frame for this player, calculate speed\n",
    "#                     if i not in prev_positions:\n",
    "#                         prev_positions[i] = current_position\n",
    "#                         speed = 0\n",
    "#                     else:\n",
    "#                         prev_position = prev_positions[i]\n",
    "#                         distance = calculate_distance(current_position, prev_position)\n",
    "#                         time_diff = 1 / fps  # Time between frames (in seconds)\n",
    "#                         speed = distance / time_diff  # Speed = distance / time\n",
    "\n",
    "#                     prev_positions[i] = current_position  # Update the previous position\n",
    "\n",
    "#                     # Recognize action and provide feedback\n",
    "#                     action, feedback = recognize_action(landmarks_person)\n",
    "#                     if action != \"Action: Unknown\":\n",
    "#                         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#                         actions_feedback.append([timestamp, \"Video\", frame_number, f\"Player {i+1}: {action}\", feedback])\n",
    "\n",
    "#                     # Drawing landmarks for each player detected\n",
    "#                     mp_drawing.draw_landmarks(frame, landmarks_person, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "#                     # Display speed next to each player\n",
    "#                     text_speed = f'Speed: {speed:.2f} m/s'\n",
    "#                     text_action = f'Action: {action}'\n",
    "\n",
    "#                     # Draw speed and action text near the player (torso center position)\n",
    "#                     cv2.putText(frame, text_speed, \n",
    "#                                 (int(torso_center_x * frame.shape[1]), int(torso_center_y * frame.shape[0] - 20)), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "#                     cv2.putText(frame, text_action, \n",
    "#                                 (int(torso_center_x * frame.shape[1]), int(torso_center_y * frame.shape[0] + 20)), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#             else:\n",
    "#                 # If only one person is detected, process this as a single NormalizedLandmarkList\n",
    "#                 action, feedback = recognize_action(result.pose_landmarks)\n",
    "#                 if action != \"Action: Unknown\":\n",
    "#                     timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#                     actions_feedback.append([timestamp, \"Video\", frame_number, action, feedback])\n",
    "\n",
    "#                 mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "#                 # Calculate the speed of the player (similar to above)\n",
    "#                 left_shoulder = result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "#                 right_shoulder = result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "#                 left_hip = result.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "#                 right_hip = result.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "\n",
    "#                 torso_center_x = (left_shoulder.x + right_shoulder.x + left_hip.x + right_hip.x) / 4\n",
    "#                 torso_center_y = (left_shoulder.y + right_shoulder.y + left_hip.y + right_hip.y) / 4\n",
    "#                 current_position = (torso_center_x, torso_center_y)\n",
    "\n",
    "#                 if 'prev_position' in locals():\n",
    "#                     distance = calculate_distance(current_position, prev_position)\n",
    "#                     time_diff = 1 / fps\n",
    "#                     speed = distance / time_diff\n",
    "#                 else:\n",
    "#                     speed = 0\n",
    "\n",
    "#                 prev_position = current_position\n",
    "\n",
    "#                 # Display speed and action next to the player\n",
    "#                 text_speed = f'Speed: {speed:.2f} m/s'\n",
    "#                 text_action = f'Action: {action}'\n",
    "\n",
    "#                 # Draw speed and action text near the player (torso center position)\n",
    "#                 cv2.putText(frame, text_speed, \n",
    "#                             (int(torso_center_x * frame.shape[1]), int(torso_center_y * frame.shape[0] - 20)), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "#                 cv2.putText(frame, text_action, \n",
    "#                             (int(torso_center_x * frame.shape[1]), int(torso_center_y * frame.shape[0] + 20)), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#         cv2.imshow(\"Pose Estimation\", frame)\n",
    "\n",
    "#         # Wait for 'q' key to exit\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "#     # After video ends, display the result in the treeview\n",
    "#     for row in actions_feedback:\n",
    "#         treeview.insert(\"\", \"end\", values=row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\dhira/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-11-7 Python-3.12.3 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS of the video: 25.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Initialize Mediapipe pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Initialize drawing utilities for Mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load YOLOv5 model (make sure you have YOLOv5 installed and configured)\n",
    "yolo_model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", force_reload=True)  # Load pre-trained YOLOv5 small model\n",
    "\n",
    "# Calculate angle helper function\n",
    "def calculate_angle(point1, point2, point3):\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    x3, y3 = point3\n",
    "    angle = np.degrees(np.arctan2(y3 - y2, x3 - x2) - np.arctan2(y1 - y2, x1 - x2))\n",
    "    return abs(angle) if angle >= 0 else abs(angle + 360)\n",
    "\n",
    "# Recognize Action (based on shoulder angle or other criteria)\n",
    "def recognize_action(landmarks):\n",
    "    if landmarks is not None:\n",
    "        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        left_wrist = landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "        shoulder_angle = calculate_angle(\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (left_elbow.x, left_elbow.y),\n",
    "            (left_wrist.x, left_wrist.y)\n",
    "        )\n",
    "\n",
    "        if shoulder_angle < 30:\n",
    "            action = \"Shooting\"\n",
    "            feedback = \"Improve shooting accuracy.\"\n",
    "        elif 30 <= shoulder_angle < 45:\n",
    "            action = \"Dribbling\"\n",
    "            feedback = \"Maintain low center of gravity.\"\n",
    "        elif 45 <= shoulder_angle < 75:\n",
    "            action = \"Standing\"\n",
    "            feedback = \"Maintain balance and posture.\"\n",
    "        elif 75 <= shoulder_angle < 105:\n",
    "            action = \"Jumping\"\n",
    "            feedback = \"Enhance explosive power and landing.\"\n",
    "        elif 105 <= shoulder_angle < 130:\n",
    "            action = \"Bending\"\n",
    "            feedback = \"Align knees and back properly.\"\n",
    "        elif 130 <= shoulder_angle < 150:\n",
    "            action = \"Walking\"\n",
    "            feedback = \"Focus on smooth strides.\"\n",
    "        elif 150 <= shoulder_angle <= 180:\n",
    "            action = \"Running\"\n",
    "            feedback = \"Keep steady pace and form.\"\n",
    "        else:\n",
    "            action = \"Unknown\"\n",
    "            feedback = \"Keep working on form.\"\n",
    "    else:\n",
    "        action = \"Unknown\"\n",
    "        feedback = \"No pose detected.\"\n",
    "\n",
    "    return action, feedback\n",
    "\n",
    "# Calculate the distance between two points\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "\n",
    "# Function for performing pose estimation on each frame\n",
    "def pose_estimation_on_frame(frame, bbox):\n",
    "    # Crop the frame to the player's bounding box\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    cropped_frame = frame[y1:y2, x1:x2]\n",
    "    cropped_frame_rgb = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose.process(cropped_frame_rgb)\n",
    "    return result\n",
    "\n",
    "# Function to analyze video and track multiple players\n",
    "def analyze_video(video_path, treeview):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_number = 0\n",
    "    prev_positions = {}  # To store the previous position of each player\n",
    "    actions_feedback = []  # To store actions and feedback for later display\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        messagebox.showerror(\"Error\", \"Could not open video file.\")\n",
    "        return\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get the frames per second of the video\n",
    "    print(f\"FPS of the video: {fps}\")  # Check FPS to ensure it matches the expected speed\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Break if no frame is returned (end of video)\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "        # Use YOLOv5 to detect players (bounding boxes)\n",
    "        results = yolo_model(frame)\n",
    "        bbox_list = results.xywh[0].cpu().numpy()  # Get detected bounding boxes\n",
    "\n",
    "        for bbox in bbox_list:\n",
    "            # Get bounding box coordinates\n",
    "            x1, y1, w, h, conf, cls = bbox[:6]\n",
    "            if conf > 0.5:  # Only consider detections with high confidence\n",
    "                x1, y1, x2, y2 = int(x1 - w / 2), int(y1 - h / 2), int(x1 + w / 2), int(y1 + h / 2)\n",
    "\n",
    "                # Perform pose estimation within the bounding box\n",
    "                result = pose_estimation_on_frame(frame, (x1, y1, x2, y2))\n",
    "\n",
    "                if result.pose_landmarks:\n",
    "                    action, feedback = recognize_action(result.pose_landmarks)\n",
    "\n",
    "                    # Only add action once per player in the frame\n",
    "                    if action != \"Unknown\":\n",
    "                        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        actions_feedback.append([timestamp, \"Video\", frame_number, f\"Player: {action}\", feedback])\n",
    "\n",
    "                    # Calculate player speed (based on center of the bounding box)\n",
    "                    torso_center_x = (x1 + x2) / 2\n",
    "                    torso_center_y = (y1 + y2) / 2\n",
    "                    current_position = (torso_center_x, torso_center_y)\n",
    "\n",
    "                    # Calculate speed in pixels per frame\n",
    "                    if frame_number > 1:\n",
    "                        prev_position = prev_positions.get(frame_number - 1, current_position)\n",
    "                        distance = calculate_distance(current_position, prev_position)\n",
    "                        time_diff = 1 / fps  # Time per frame\n",
    "\n",
    "                        # Convert pixel distance to meters (calibration factor needed)\n",
    "                        pixel_to_meter_factor = 0.05  # 1 pixel = 0.05 meters (calibrate this factor)\n",
    "                        speed_in_mps = (distance * pixel_to_meter_factor) / time_diff  # Speed in meters per second\n",
    "\n",
    "                        # Convert meters per second to kilometers per hour (1 m/s = 3.6 km/h)\n",
    "                        speed_in_kmh = speed_in_mps * 3.6\n",
    "                    else:\n",
    "                        speed_in_kmh = 0\n",
    "\n",
    "                    prev_positions[frame_number] = current_position\n",
    "\n",
    "                    # Display speed and action near the player\n",
    "                    text_speed = f'Speed: {speed_in_kmh:.2f} km/h'\n",
    "                    text_action = f'Action: {action}'\n",
    "\n",
    "                    # Draw speed and action text near the player (center position)\n",
    "                    cv2.putText(frame, text_speed, \n",
    "                                (int(torso_center_x), int(torso_center_y - 20)), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "                    cv2.putText(frame, text_action, \n",
    "                                (int(torso_center_x), int(torso_center_y + 20)), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the frame in a window\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Adjust the wait time to match real-time playback speed (no delay, or minimal delay for normal playback speed)\n",
    "        key = cv2.waitKey(10)  # Skip frames if you want to process faster\n",
    "        if key == 27:  # Press 'Esc' to exit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Show collected actions and feedback in the Tkinter treeview\n",
    "    for action_feedback in actions_feedback:\n",
    "        treeview.insert(\"\", \"end\", values=action_feedback)\n",
    "\n",
    "# Function to browse video file\n",
    "def browse_video_file(treeview):\n",
    "    video_path = filedialog.askopenfilename(title=\"Select a video file\", filetypes=[(\"Video files\", \"*.mp4 *.avi *.mov\")])\n",
    "    if video_path:\n",
    "        analyze_video(video_path, treeview)\n",
    "\n",
    "# Tkinter setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Player Speed and Action Analysis\")\n",
    "\n",
    "# Create treeview widget\n",
    "treeview = ttk.Treeview(root, columns=(\"Timestamp\", \"Type\", \"Frame Number\", \"Action\", \"Feedback\"), show=\"headings\")\n",
    "treeview.heading(\"Timestamp\", text=\"Timestamp\")\n",
    "treeview.heading(\"Type\", text=\"Type\")\n",
    "treeview.heading(\"Frame Number\", text=\"Frame Number\")\n",
    "treeview.heading(\"Action\", text=\"Action\")\n",
    "treeview.heading(\"Feedback\", text=\"Feedback\")\n",
    "treeview.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Browse button to load the video\n",
    "browse_button = tk.Button(root, text=\"Browse Video\", command=lambda: browse_video_file(treeview))\n",
    "browse_button.pack(pady=10)\n",
    "\n",
    "root.mainloop()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
