{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 1:  Video Upload and Frame Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Directory for video uplaod and frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame extraction completed. Frames are saved in Video2_frames\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to create directory for storing frames\n",
    "def create_frame_dir(video_file):\n",
    "    frame_dir = os.path.splitext(video_file)[0] + \"_frames\"\n",
    "    if not os.path.exists(frame_dir):\n",
    "        os.makedirs(frame_dir)\n",
    "    return frame_dir\n",
    "\n",
    "# Function to extract and save frames from the video\n",
    "def extract_frames(video_file):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    # Create directory to store the frames\n",
    "    frame_dir = create_frame_dir(video_file)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # If no frame is captured, exit\n",
    "        \n",
    "        # Save frame as image\n",
    "        frame_path = os.path.join(frame_dir, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Extracted frame {frame_count}\")\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Frame extraction completed. Frames are saved in {frame_dir}\")\n",
    "\n",
    "# Call the function with your video file path\n",
    "video_file = \"Video2.mp4\"  # Replace with your video file\n",
    "extract_frames(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Player Detection Using YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use YOLOv5 to detect players in frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dhira/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-5 Python-3.12.3 torch-2.4.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load YOLOv5 pre-trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Detect players in the extracted frames\n",
    "def detect_players_in_frames(frame_dir):\n",
    "    frame_paths = list(Path(frame_dir).glob('*.jpg'))\n",
    "    \n",
    "    for frame_path in frame_paths:\n",
    "        # Perform detection\n",
    "        results = model(frame_path)\n",
    "        \n",
    "        # Display results (bounding boxes around detected players)\n",
    "        # results.show()  # This will show the image with detection\n",
    "\n",
    "        # If you want to save the results, uncomment the following line\n",
    "        # results.save()  # This will save images with bounding boxes to the current directory\n",
    "        \n",
    "        # print(f\"Detected players in {frame_path}\")\n",
    "\n",
    "# Call the function with the frame directory\n",
    "frame_dir = os.path.splitext(video_file)[0] + \"_frames\"  # Replace with your frame directory\n",
    "detect_players_in_frames(frame_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tracking the Player Across Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "scikit-learn version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Player tracking with DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dhira/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-5 Python-3.12.3 torch-2.4.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the YOLOv5 model (Ensure that it's already set up)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Assuming the DeepSORT object is replaced with a custom tracker or placeholder\n",
    "class SimpleTracker:\n",
    "    def __init__(self):\n",
    "        self.next_id = 0\n",
    "        self.objects = {}\n",
    "\n",
    "    def update(self, bboxes):\n",
    "        new_objects = {}\n",
    "        for bbox in bboxes:\n",
    "            new_objects[self.next_id] = bbox\n",
    "            self.next_id += 1\n",
    "        self.objects = new_objects\n",
    "        return [[*bbox, obj_id] for obj_id, bbox in self.objects.items()]\n",
    "\n",
    "# Initialize a simple tracker\n",
    "tracker = SimpleTracker()\n",
    "\n",
    "# Function to track players across frames\n",
    "def track_players_in_video(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit if no frame is captured\n",
    "\n",
    "        # Perform detection (similar to part 2)\n",
    "        results = model(frame)\n",
    "        bboxes = results.xyxy[0][:, :4].cpu().numpy()  # Bounding boxes\n",
    "        confs = results.xyxy[0][:, 4].cpu().numpy()    # Confidence scores\n",
    "\n",
    "        # Perform tracking using a simple tracker (replace deepsort)\n",
    "        outputs = tracker.update(bboxes)\n",
    "\n",
    "        # Draw the bounding boxes and track IDs on the frame\n",
    "        for output in outputs:\n",
    "            x1, y1, x2, y2, track_id = output[:5]\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"ID: {track_id}\", (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Tracking\", frame)\n",
    "        frame_count += 1\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the tracking function\n",
    "video_file = \"Video2.mp4\"  # Replace with your video file\n",
    "track_players_in_video(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Pose Estimation Using OpenPose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Implement Pose Estimation on Extracted Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dhira/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-5 Python-3.12.3 torch-2.4.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Load YOLOv5 pre-trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Function to perform pose estimation on a single frame\n",
    "def pose_estimation_on_frame(frame):\n",
    "    # Convert frame to RGB as MediaPipe works with RGB images\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform pose estimation\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Draw keypoints on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(annotated_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    return annotated_frame, results.pose_landmarks\n",
    "\n",
    "# Function to detect players and perform pose estimation\n",
    "def process_video(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit if no frame is captured\n",
    "\n",
    "        # Perform detection using YOLOv5\n",
    "        results = model(frame)\n",
    "        bboxes = results.xyxy[0].cpu().numpy()  # Bounding boxes\n",
    "\n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Draw bounding boxes for detected players\n",
    "        for bbox in bboxes:\n",
    "            # Check the number of values in bbox\n",
    "            if len(bbox) == 4:\n",
    "                x1, y1, x2, y2 = bbox  # If only bounding box coordinates are returned\n",
    "                conf = 1.0  # Set default confidence if not available\n",
    "            elif len(bbox) >= 5:\n",
    "                x1, y1, x2, y2, conf = bbox[:5]  # Get the first five values\n",
    "            else:\n",
    "                continue  # Skip if bbox has fewer than 4 values\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "\n",
    "        # Display the annotated frame with pose landmarks and bounding boxes\n",
    "        cv2.imshow(\"Pose Estimation and Player Detection\", annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function with your video file\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_video(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define Actions and Recognition Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to calculate the angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)  # First\n",
    "    b = np.array(b)  # Mid\n",
    "    c = np.array(c)  # End\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "    return angle\n",
    "\n",
    "# Function to perform pose estimation on a single frame\n",
    "def pose_estimation_on_frame(frame):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(annotated_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    return annotated_frame, results.pose_landmarks\n",
    "\n",
    "# Action recognition function\n",
    "def recognize_action(landmarks):\n",
    "    if landmarks is not None:\n",
    "        # Define keypoints\n",
    "        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        left_wrist = landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "        \n",
    "        right_shoulder = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        right_elbow = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "        right_wrist = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "        left_hip = landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "        left_knee = landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "        left_ankle = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "\n",
    "        right_hip = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "        right_knee = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "        right_ankle = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
    "\n",
    "        # Calculate angles\n",
    "        shoulder_angle_left = calculate_angle(\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (left_elbow.x, left_elbow.y),\n",
    "            (left_wrist.x, left_wrist.y)\n",
    "        )\n",
    "\n",
    "        shoulder_angle_right = calculate_angle(\n",
    "            (right_shoulder.x, right_shoulder.y),\n",
    "            (right_elbow.x, right_elbow.y),\n",
    "            (right_wrist.x, right_wrist.y)\n",
    "        )\n",
    "\n",
    "        hip_angle_left = calculate_angle(\n",
    "            (left_hip.x, left_hip.y),\n",
    "            (left_knee.x, left_knee.y),\n",
    "            (left_ankle.x, left_ankle.y)\n",
    "        )\n",
    "\n",
    "        hip_angle_right = calculate_angle(\n",
    "            (right_hip.x, right_hip.y),\n",
    "            (right_knee.x, right_knee.y),\n",
    "            (right_ankle.x, right_ankle.y)\n",
    "        )\n",
    "\n",
    "        # Debug outputs for angles\n",
    "        print(f\"Left Shoulder Angle: {shoulder_angle_left}\")\n",
    "        print(f\"Right Shoulder Angle: {shoulder_angle_right}\")\n",
    "        print(f\"Left Hip Angle: {hip_angle_left}\")\n",
    "        print(f\"Right Hip Angle: {hip_angle_right}\")\n",
    "\n",
    "        # Define action recognition based on calculated angles\n",
    "        if shoulder_angle_left < 30 and shoulder_angle_right < 30:  # Shooting\n",
    "            return \"Action: Shooting\"\n",
    "        elif shoulder_angle_left > 150 and shoulder_angle_right > 150:  # Running\n",
    "            return \"Action: Running\"\n",
    "        elif 30 <= shoulder_angle_left <= 150 and 30 <= shoulder_angle_right <= 150:  # Neutral position\n",
    "            return \"Action: Neutral\"\n",
    "        elif hip_angle_left < 30 and hip_angle_right < 30:  # Jumping\n",
    "            return \"Action: Jumping\"\n",
    "        elif hip_angle_left > 160 and hip_angle_right > 160:  # Standing\n",
    "            return \"Action: Standing\"\n",
    "        elif 60 <= hip_angle_left <= 120 and 60 <= hip_angle_right <= 120:  # Bending\n",
    "            return \"Action: Bending\"\n",
    "        elif 45 <= hip_angle_left <= 90 and 45 <= hip_angle_right <= 90:  # Walking\n",
    "            return \"Action: Walking\"\n",
    "        else:\n",
    "            return \"Action: Unknown\"  # Default case\n",
    "\n",
    "    return \"Action: Unknown\"\n",
    "\n",
    "# Process all frames for pose estimation and action recognition\n",
    "def process_pose_estimation_on_video_with_action_recognition(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Debug output for landmarks\n",
    "        if landmarks:\n",
    "            for idx, landmark in enumerate(landmarks.landmark):\n",
    "                print(f\"Landmark {idx}: ({landmark.x}, {landmark.y})\")\n",
    "\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the action recognition function with your video\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_action_recognition(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Generating Feedback for Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Generate Feedback Based on Recognized Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: Generating Feedback for Players\n",
    "\n",
    "def generate_feedback(action):\n",
    "    feedback = \"\"\n",
    "    \n",
    "    # Provide feedback based on the recognized action\n",
    "    if action == \"Action: Shooting\":\n",
    "        feedback = \"Feedback: Focus on improving your shooting technique. Aim for accuracy.\"\n",
    "    elif action == \"Action: Running\":\n",
    "        feedback = \"Feedback: Maintain a steady pace and good form while running.\"\n",
    "    elif action == \"Action: Neutral\":\n",
    "        feedback = \"Feedback: Keep practicing your movements for better fluidity.\"\n",
    "    elif action == \"Action: Jumping\":\n",
    "        feedback = \"Feedback: Focus on explosive power and proper landing technique.\"\n",
    "    elif action == \"Action: Standing\":\n",
    "        feedback = \"Feedback: Maintain a balanced and stable stance. Avoid slouching.\"\n",
    "    elif action == \"Action: Bending\":\n",
    "        feedback = \"Feedback: Ensure your knees are aligned with your toes and your back is straight.\"\n",
    "    elif action == \"Action: Walking\":\n",
    "        feedback = \"Feedback: Practice smooth and controlled strides to enhance your walking efficiency.\"\n",
    "    else:\n",
    "        feedback = \"Feedback: Keep practicing your actions for better performance.\"\n",
    "    \n",
    "    return feedback\n",
    "\n",
    "# Modify the video processing function to include feedback generation\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the feedback generation function with your video\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Simple Implementation of getting video and anlysis them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback for Video2.mp4: Analysis completed for Video2.mp4\n"
     ]
    }
   ],
   "source": [
    "def analyze_video(video_path):\n",
    "    # Placeholder for video analysis logic\n",
    "    # Replace this with actual video analysis code\n",
    "    # For now, let's just return a dummy feedback\n",
    "    return f\"Analysis completed for {video_path}\"\n",
    "\n",
    "def test_videos(video_list):\n",
    "    results = []\n",
    "    for video_path in video_list:\n",
    "        feedback = analyze_video(video_path)  # Call the video analysis function\n",
    "        results.append((video_path, feedback))\n",
    "    return results\n",
    "\n",
    "# Sample usage with your specified video file\n",
    "video_list = ['Video2.mp4']  # Use your specific video file\n",
    "results = test_videos(video_list)\n",
    "for video, feedback in results:\n",
    "    print(f\"Feedback for {video}: {feedback}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_threshold = 0.5  # Initial threshold\n",
    "\n",
    "def set_action_threshold(new_threshold):\n",
    "    global action_threshold\n",
    "    action_threshold = new_threshold\n",
    "\n",
    "# Example of updating the threshold\n",
    "set_action_threshold(0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to log feedback for each frame with timestamp\n",
    "def log_feedback(video_path, action, feedback, frame_number):\n",
    "    with open('feedback_log.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([datetime.now(), video_path, frame_number, action, feedback])\n",
    "\n",
    "# Example of how the function would be used within your video processing loop\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_number = 0  # Initialize frame counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Log the feedback for this frame\n",
    "        log_feedback(video_file, action, feedback, frame_number)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        # Increment frame number\n",
    "        frame_number += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Sample logging within video processing function\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to initialize the database with the appropriate table\n",
    "def initialize_db():\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS performance (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            timestamp TEXT,\n",
    "            video_path TEXT,\n",
    "            frame_number INTEGER,\n",
    "            action TEXT,\n",
    "            feedback TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to insert performance data into the database\n",
    "def insert_performance(video_path, frame_number, action, feedback):\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        INSERT INTO performance (timestamp, video_path, frame_number, action, feedback) \n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    ''', (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), video_path, frame_number, action, feedback))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Initialize the database\n",
    "initialize_db()\n",
    "\n",
    "# Modified video processing function to log actions and feedback frame by frame\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_number = 0  # Initialize frame counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Insert the feedback for this frame into the database\n",
    "        insert_performance(video_file, frame_number, action, feedback)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        # Increment frame number\n",
    "        frame_number += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the video processing function\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Initialize database and ensure all necessary columns exist\n",
    "def initialize_db():\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create the performance table if it does not exist\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS performance (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        timestamp TEXT,\n",
    "                        video_path TEXT,\n",
    "                        frame_number INTEGER,\n",
    "                        action TEXT,\n",
    "                        feedback TEXT)''')\n",
    "    \n",
    "    # Add missing columns if necessary\n",
    "    cursor.execute('''PRAGMA table_info(performance)''')\n",
    "    columns = [column[1] for column in cursor.fetchall()]\n",
    "\n",
    "    # Check and add missing columns\n",
    "    if 'frame_number' not in columns:\n",
    "        cursor.execute('''ALTER TABLE performance ADD COLUMN frame_number INTEGER''')\n",
    "    if 'action' not in columns:\n",
    "        cursor.execute('''ALTER TABLE performance ADD COLUMN action TEXT''')\n",
    "    if 'feedback' not in columns:\n",
    "        cursor.execute('''ALTER TABLE performance ADD COLUMN feedback TEXT''')\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Insert performance into the database (not really needed in this version, but keeping for structure)\n",
    "def insert_performance(timestamp, video_path, frame_number, action, feedback):\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''INSERT INTO performance (timestamp, video_path, frame_number, action, feedback) \n",
    "                      VALUES (?, ?, ?, ?, ?)''', (timestamp, video_path, frame_number, action, feedback))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Pose Estimation function\n",
    "def pose_estimation_on_frame(frame):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "    return results.pose_landmarks\n",
    "\n",
    "# Recognize Action (based on shoulder angle or other criteria)\n",
    "def recognize_action(landmarks):\n",
    "    if landmarks is not None:\n",
    "        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        left_wrist = landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "        # Example of calculating shoulder angle to determine the action\n",
    "        shoulder_angle = calculate_angle(\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (left_elbow.x, left_elbow.y),\n",
    "            (left_wrist.x, left_wrist.y)\n",
    "        )\n",
    "\n",
    "        # Determine the action based on shoulder angle or other criteria\n",
    "        if shoulder_angle < 45:\n",
    "            action = \"Action: Shooting\"\n",
    "        elif shoulder_angle > 135:\n",
    "            action = \"Action: Running\"\n",
    "        elif shoulder_angle > 75 and shoulder_angle < 105:\n",
    "            action = \"Action: Jumping\"\n",
    "        elif shoulder_angle > 45 and shoulder_angle < 75:\n",
    "            action = \"Action: Standing\"\n",
    "        elif shoulder_angle > 105 and shoulder_angle < 130:\n",
    "            action = \"Action: Bending\"\n",
    "        elif shoulder_angle > 130 and shoulder_angle < 150:\n",
    "            action = \"Action: Walking\"\n",
    "        else:\n",
    "            action = \"Action: Unknown\"\n",
    "    else:\n",
    "        action = \"Action: Unknown\"\n",
    "\n",
    "    # Provide feedback based on the recognized action\n",
    "    if action == \"Action: Shooting\":\n",
    "        feedback = \"Feedback: Improve shooting accuracy\"\n",
    "    elif action == \"Action: Running\":\n",
    "        feedback = \"Feedback: Keep steady pace and form.\"\n",
    "    elif action == \"Action: Neutral\":\n",
    "        feedback = \"Feedback: Practice for smoother movements.\"\n",
    "    elif action == \"Action: Jumping\":\n",
    "        feedback = \"Feedback: Enhance explosive power and landing.\"\n",
    "    elif action == \"Action: Standing\":\n",
    "        feedback = \"Feedback: Maintain balance and posture.\"\n",
    "    elif action == \"Action: Bending\":\n",
    "        feedback = \"Feedback: Align knees and back properly.\"\n",
    "    elif action == \"Action: Walking\":\n",
    "        feedback = \"Feedback: Focus on smooth strides..\"\n",
    "    else:\n",
    "        feedback = \"Feedback: Keep working on.\"\n",
    "\n",
    "    return action, feedback\n",
    "\n",
    "# Calculate the angle between 3 points\n",
    "def calculate_angle(a, b, c):\n",
    "    import math\n",
    "    ang = math.degrees(math.atan2(c[1] - b[1], c[0] - b[0]) - math.atan2(a[1] - b[1], a[0] - b[0]))\n",
    "    if ang < 0:\n",
    "        ang += 360\n",
    "    return ang\n",
    "\n",
    "# Function to process the video and insert analysis results into the database\n",
    "def process_video_with_feedback(video_file, treeview):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_number = 0\n",
    "    results_list = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_number += 1\n",
    "        landmarks = pose_estimation_on_frame(frame)\n",
    "        action, feedback = recognize_action(landmarks)\n",
    "\n",
    "        # Skip adding results with \"Unknown\" action\n",
    "        if action == \"Action: Unknown\":\n",
    "            continue\n",
    "\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Append the result for display in the table\n",
    "        results_list.append([timestamp, video_file, frame_number, action, feedback])\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Clear any previous data in the Treeview and insert the results\n",
    "    for row in treeview.get_children():\n",
    "        treeview.delete(row)\n",
    "\n",
    "    # Insert only valid results (excluding Unknown actions)\n",
    "    for result in results_list:\n",
    "        treeview.insert(\"\", tk.END, values=result)\n",
    "\n",
    "    # Show the results in a message box after processing\n",
    "    messagebox.showinfo(\"Analysis Complete\", \"The video has been analyzed successfully.\")\n",
    "\n",
    "# Function to select video and analyze\n",
    "def analyze_video(treeview):\n",
    "    video_file = filedialog.askopenfilename(title=\"Select a Video\", filetypes=[(\"MP4 files\", \"*.mp4\"), (\"All files\", \"*.*\")])\n",
    "    if video_file:\n",
    "        process_video_with_feedback(video_file, treeview)\n",
    "\n",
    "# Create the Tkinter GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Player Performance Analysis\")\n",
    "root.geometry(\"1000x700\")  # Set window size\n",
    "\n",
    "# Initialize the database\n",
    "initialize_db()\n",
    "\n",
    "# Add a frame for the Treeview (table) to display the results\n",
    "frame = tk.Frame(root, bg=\"#f0f0f0\")\n",
    "frame.pack(padx=20, pady=20, fill=\"both\", expand=True)\n",
    "\n",
    "# Set up the Treeview (table) to show the results\n",
    "columns = [\"Timestamp\", \"Video Path\", \"Frame Number\", \"Action\", \"Feedback\"]\n",
    "treeview = ttk.Treeview(frame, columns=columns, show=\"headings\", height=10)\n",
    "for col in columns:\n",
    "    treeview.heading(col, text=col)\n",
    "    treeview.column(col, width=200, anchor=\"center\")\n",
    "\n",
    "treeview.pack(pady=10, fill=\"both\", expand=True)\n",
    "\n",
    "# Add a button to start the video analysis\n",
    "analyze_button = tk.Button(root, text=\"Analyze Video\", command=lambda: analyze_video(treeview), bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "analyze_button.pack(pady=20)\n",
    "\n",
    "# Set some colors for better UI experience\n",
    "root.configure(bg=\"#f0f0f0\")\n",
    "root.option_add(\"*TButton.padding\", [10, 5])  # Padding around buttons\n",
    "root.option_add(\"*Font\", \"Arial 10\")  # Default font\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Tkinter event loop\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
