{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 1:  Video Upload and Frame Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Directory for video uplaod and frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame extraction completed. Frames are saved in Video2_frames\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to create directory for storing frames\n",
    "def create_frame_dir(video_file):\n",
    "    frame_dir = os.path.splitext(video_file)[0] + \"_frames\"\n",
    "    if not os.path.exists(frame_dir):\n",
    "        os.makedirs(frame_dir)\n",
    "    return frame_dir\n",
    "\n",
    "# Function to extract and save frames from the video\n",
    "def extract_frames(video_file):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    # Create directory to store the frames\n",
    "    frame_dir = create_frame_dir(video_file)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # If no frame is captured, exit\n",
    "        \n",
    "        # Save frame as image\n",
    "        frame_path = os.path.join(frame_dir, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Extracted frame {frame_count}\")\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Frame extraction completed. Frames are saved in {frame_dir}\")\n",
    "\n",
    "# Call the function with your video file path\n",
    "video_file = \"Video2.mp4\"  # Replace with your video file\n",
    "extract_frames(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Player Detection Using YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use YOLOv5 to detect players in frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\vimmy/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-20 Python-3.12.6 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load YOLOv5 pre-trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Detect players in the extracted frames\n",
    "def detect_players_in_frames(frame_dir):\n",
    "    frame_paths = list(Path(frame_dir).glob('*.jpg'))\n",
    "    \n",
    "    for frame_path in frame_paths:\n",
    "        # Perform detection\n",
    "        results = model(frame_path)\n",
    "        \n",
    "        # Display results (bounding boxes around detected players)\n",
    "        # results.show()  # This will show the image with detection\n",
    "\n",
    "        # If you want to save the results, uncomment the following line\n",
    "        # results.save()  # This will save images with bounding boxes to the current directory\n",
    "        \n",
    "        # print(f\"Detected players in {frame_path}\")\n",
    "\n",
    "# Call the function with the frame directory\n",
    "frame_dir = os.path.splitext(video_file)[0] + \"_frames\"  # Replace with your frame directory\n",
    "detect_players_in_frames(frame_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tracking the Player Across Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "scikit-learn version: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Player tracking with DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\vimmy/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-20 Python-3.12.6 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the YOLOv5 model (Ensure that it's already set up)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Assuming the DeepSORT object is replaced with a custom tracker or placeholder\n",
    "class SimpleTracker:\n",
    "    def __init__(self):\n",
    "        self.next_id = 0\n",
    "        self.objects = {}\n",
    "\n",
    "    def update(self, bboxes):\n",
    "        new_objects = {}\n",
    "        for bbox in bboxes:\n",
    "            new_objects[self.next_id] = bbox\n",
    "            self.next_id += 1\n",
    "        self.objects = new_objects\n",
    "        return [[*bbox, obj_id] for obj_id, bbox in self.objects.items()]\n",
    "\n",
    "# Initialize a simple tracker\n",
    "tracker = SimpleTracker()\n",
    "\n",
    "# Function to track players across frames\n",
    "def track_players_in_video(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit if no frame is captured\n",
    "\n",
    "        # Perform detection (similar to part 2)\n",
    "        results = model(frame)\n",
    "        bboxes = results.xyxy[0][:, :4].cpu().numpy()  # Bounding boxes\n",
    "        confs = results.xyxy[0][:, 4].cpu().numpy()    # Confidence scores\n",
    "\n",
    "        # Perform tracking using a simple tracker (replace deepsort)\n",
    "        outputs = tracker.update(bboxes)\n",
    "\n",
    "        # Draw the bounding boxes and track IDs on the frame\n",
    "        for output in outputs:\n",
    "            x1, y1, x2, y2, track_id = output[:5]\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"ID: {track_id}\", (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Tracking\", frame)\n",
    "        frame_count += 1\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "   \n",
    "\n",
    "# Call the tracking function\n",
    "video_file = \"Video2.mp4\"  # Replace with your video file\n",
    "track_players_in_video(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Pose Estimation Using OpenPose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Implement Pose Estimation on Extracted Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\vimmy/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-20 Python-3.12.6 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Load YOLOv5 pre-trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Function to perform pose estimation on a single frame\n",
    "def pose_estimation_on_frame(frame):\n",
    "    # Convert frame to RGB as MediaPipe works with RGB images\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform pose estimation\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Draw keypoints on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(annotated_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    return annotated_frame, results.pose_landmarks\n",
    "\n",
    "# Function to detect players and perform pose estimation\n",
    "def process_video(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit if no frame is captured\n",
    "\n",
    "        # Perform detection using YOLOv5\n",
    "        results = model(frame)\n",
    "        bboxes = results.xyxy[0].cpu().numpy()  # Bounding boxes\n",
    "\n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Draw bounding boxes for detected players\n",
    "        for bbox in bboxes:\n",
    "            # Check the number of values in bbox\n",
    "            if len(bbox) == 4:\n",
    "                x1, y1, x2, y2 = bbox  # If only bounding box coordinates are returned\n",
    "                conf = 1.0  # Set default confidence if not available\n",
    "            elif len(bbox) >= 5:\n",
    "                x1, y1, x2, y2, conf = bbox[:5]  # Get the first five values\n",
    "            else:\n",
    "                continue  # Skip if bbox has fewer than 4 values\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(annotated_frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "\n",
    "        # Display the annotated frame with pose landmarks and bounding boxes\n",
    "        cv2.imshow(\"Pose Estimation and Player Detection\", annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function with your video file\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_video(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define Actions and Recognition Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to calculate the angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)  # First\n",
    "    b = np.array(b)  # Mid\n",
    "    c = np.array(c)  # End\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "    return angle\n",
    "\n",
    "# Function to perform pose estimation on a single frame\n",
    "def pose_estimation_on_frame(frame):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(annotated_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    return annotated_frame, results.pose_landmarks\n",
    "\n",
    "# Action recognition function\n",
    "def recognize_action(landmarks):\n",
    "    if landmarks is not None:\n",
    "        # Define keypoints\n",
    "        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        left_wrist = landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "        \n",
    "        right_shoulder = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        right_elbow = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW]\n",
    "        right_wrist = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "        left_hip = landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP]\n",
    "        left_knee = landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "        left_ankle = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ANKLE]\n",
    "\n",
    "        right_hip = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "        right_knee = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE]\n",
    "        right_ankle = landmarks.landmark[mp_pose.PoseLandmark.RIGHT_ANKLE]\n",
    "\n",
    "        # Calculate angles\n",
    "        shoulder_angle_left = calculate_angle(\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (left_elbow.x, left_elbow.y),\n",
    "            (left_wrist.x, left_wrist.y)\n",
    "        )\n",
    "\n",
    "        shoulder_angle_right = calculate_angle(\n",
    "            (right_shoulder.x, right_shoulder.y),\n",
    "            (right_elbow.x, right_elbow.y),\n",
    "            (right_wrist.x, right_wrist.y)\n",
    "        )\n",
    "\n",
    "        hip_angle_left = calculate_angle(\n",
    "            (left_hip.x, left_hip.y),\n",
    "            (left_knee.x, left_knee.y),\n",
    "            (left_ankle.x, left_ankle.y)\n",
    "        )\n",
    "\n",
    "        hip_angle_right = calculate_angle(\n",
    "            (right_hip.x, right_hip.y),\n",
    "            (right_knee.x, right_knee.y),\n",
    "            (right_ankle.x, right_ankle.y)\n",
    "        )\n",
    "\n",
    "        # Debug outputs for angles\n",
    "        print(f\"Left Shoulder Angle: {shoulder_angle_left}\")\n",
    "        print(f\"Right Shoulder Angle: {shoulder_angle_right}\")\n",
    "        print(f\"Left Hip Angle: {hip_angle_left}\")\n",
    "        print(f\"Right Hip Angle: {hip_angle_right}\")\n",
    "\n",
    "        # Define action recognition based on calculated angles\n",
    "        if shoulder_angle_left < 30 and shoulder_angle_right < 30:  # Shooting\n",
    "            return \"Action: Shooting\"\n",
    "        elif shoulder_angle_left > 150 and shoulder_angle_right > 150:  # Running\n",
    "            return \"Action: Running\"\n",
    "        elif 30 <= shoulder_angle_left <= 150 and 30 <= shoulder_angle_right <= 150:  # Neutral position\n",
    "            return \"Action: Neutral\"\n",
    "        elif hip_angle_left < 30 and hip_angle_right < 30:  # Jumping\n",
    "            return \"Action: Jumping\"\n",
    "        elif hip_angle_left > 160 and hip_angle_right > 160:  # Standing\n",
    "            return \"Action: Standing\"\n",
    "        elif 60 <= hip_angle_left <= 120 and 60 <= hip_angle_right <= 120:  # Bending\n",
    "            return \"Action: Bending\"\n",
    "        elif 45 <= hip_angle_left <= 90 and 45 <= hip_angle_right <= 90:  # Walking\n",
    "            return \"Action: Walking\"\n",
    "        else:\n",
    "            return \"Action: Unknown\"  # Default case\n",
    "\n",
    "    return \"Action: Unknown\"\n",
    "\n",
    "# Process all frames for pose estimation and action recognition\n",
    "def process_pose_estimation_on_video_with_action_recognition(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Debug output for landmarks\n",
    "        if landmarks:\n",
    "            for idx, landmark in enumerate(landmarks.landmark):\n",
    "                print(f\"Landmark {idx}: ({landmark.x}, {landmark.y})\")\n",
    "\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the action recognition function with your video\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_action_recognition(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Generating Feedback for Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Generate Feedback Based on Recognized Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: Generating Feedback for Players\n",
    "\n",
    "def generate_feedback(action):\n",
    "    feedback = \"\"\n",
    "    \n",
    "    # Provide feedback based on the recognized action\n",
    "    if action == \"Action: Shooting\":\n",
    "        feedback = \"Feedback: Focus on improving your shooting technique. Aim for accuracy.\"\n",
    "    elif action == \"Action: Running\":\n",
    "        feedback = \"Feedback: Maintain a steady pace and good form while running.\"\n",
    "    elif action == \"Action: Neutral\":\n",
    "        feedback = \"Feedback: Keep practicing your movements for better fluidity.\"\n",
    "    elif action == \"Action: Jumping\":\n",
    "        feedback = \"Feedback: Focus on explosive power and proper landing technique.\"\n",
    "    elif action == \"Action: Standing\":\n",
    "        feedback = \"Feedback: Maintain a balanced and stable stance. Avoid slouching.\"\n",
    "    elif action == \"Action: Bending\":\n",
    "        feedback = \"Feedback: Ensure your knees are aligned with your toes and your back is straight.\"\n",
    "    elif action == \"Action: Walking\":\n",
    "        feedback = \"Feedback: Practice smooth and controlled strides to enhance your walking efficiency.\"\n",
    "    else:\n",
    "        feedback = \"Feedback: Keep practicing your actions for better performance.\"\n",
    "    \n",
    "    return feedback\n",
    "\n",
    "# Modify the video processing function to include feedback generation\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the feedback generation function with your video\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Simple Implementation of getting video and anlysis them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback for Video2.mp4: Analysis completed for Video2.mp4\n"
     ]
    }
   ],
   "source": [
    "def analyze_video(video_path):\n",
    "    # Placeholder for video analysis logic\n",
    "    # Replace this with actual video analysis code\n",
    "    # For now, let's just return a dummy feedback\n",
    "    return f\"Analysis completed for {video_path}\"\n",
    "\n",
    "def test_videos(video_list):\n",
    "    results = []\n",
    "    for video_path in video_list:\n",
    "        feedback = analyze_video(video_path)  # Call the video analysis function\n",
    "        results.append((video_path, feedback))\n",
    "    return results\n",
    "\n",
    "# Sample usage with your specified video file\n",
    "video_list = ['Video2.mp4']  # Use your specific video file\n",
    "results = test_videos(video_list)\n",
    "for video, feedback in results:\n",
    "    print(f\"Feedback for {video}: {feedback}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_threshold = 0.5  # Initial threshold\n",
    "\n",
    "def set_action_threshold(new_threshold):\n",
    "    global action_threshold\n",
    "    action_threshold = new_threshold\n",
    "\n",
    "# Example of updating the threshold\n",
    "set_action_threshold(0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to log feedback for each frame with timestamp\n",
    "def log_feedback(video_path, action, feedback, frame_number):\n",
    "    with open('feedback_log.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([datetime.now(), video_path, frame_number, action, feedback])\n",
    "\n",
    "# Example of how the function would be used within your video processing loop\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_number = 0  # Initialize frame counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Log the feedback for this frame\n",
    "        log_feedback(video_file, action, feedback, frame_number)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        # Increment frame number\n",
    "        frame_number += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Sample logging within video processing function\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to initialize the database with the appropriate table\n",
    "def initialize_db():\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS performance (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            timestamp TEXT,\n",
    "            video_path TEXT,\n",
    "            frame_number INTEGER,\n",
    "            action TEXT,\n",
    "            feedback TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to insert performance data into the database\n",
    "def insert_performance(video_path, frame_number, action, feedback):\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        INSERT INTO performance (timestamp, video_path, frame_number, action, feedback) \n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    ''', (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), video_path, frame_number, action, feedback))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Initialize the database\n",
    "initialize_db()\n",
    "\n",
    "# Modified video processing function to log actions and feedback frame by frame\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_number = 0  # Initialize frame counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Insert the feedback for this frame into the database\n",
    "        insert_performance(video_file, frame_number, action, feedback)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        # Increment frame number\n",
    "        frame_number += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the video processing function\n",
    "video_file = \"Video2.mp4\"  # Use the video file specified earlier\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\vimmy/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-11-23 Python-3.12.6 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3': {'actions': {'Bending': ['Align knees and back properly.'], 'Running': ['Keep steady pace and form.', 'Keep steady pace and form.', 'Keep steady pace and form.'], 'Shooting': ['Improve shooting accuracy.', 'Improve shooting accuracy.', 'Improve shooting accuracy.'], 'Standing': ['Maintain balance and posture.'], 'Dribbling': ['Maintain low center of gravity.', 'Maintain low center of gravity.', 'Maintain low center of gravity.']}, 'passes': 11, 'max_speed': 25.09646409880932, 'avg_speed': 14.94258574605409, 'speed_sum': 164.368443206595, 'speed_count': 11}, '5': {'actions': {'Bending': ['Align knees and back properly.', 'Align knees and back properly.'], 'Shooting': ['Improve shooting accuracy.'], 'Jumping': ['Enhance explosive power and landing.', 'Enhance explosive power and landing.'], 'Standing': ['Maintain balance and posture.'], 'Walking': ['Focus on smooth strides.'], 'Running': ['Keep steady pace and form.']}, 'passes': 8, 'max_speed': 24.27455449393481, 'avg_speed': 15.198948073008548, 'speed_sum': 121.59158458406839, 'speed_count': 8}, '2': {'actions': {'Running': ['Keep steady pace and form.', 'Keep steady pace and form.'], 'Shooting': ['Improve shooting accuracy.']}, 'passes': 3, 'max_speed': 16.23924211043773, 'avg_speed': 10.654950713750166, 'speed_sum': 31.964852141250496, 'speed_count': 3}, '4': {'actions': {'Standing': ['Maintain balance and posture.', 'Maintain balance and posture.', 'Maintain balance and posture.', 'Maintain balance and posture.', 'Maintain balance and posture.'], 'Shooting': ['Improve shooting accuracy.', 'Improve shooting accuracy.'], 'Dribbling': ['Maintain low center of gravity.', 'Maintain low center of gravity.'], 'Bending': ['Align knees and back properly.'], 'Jumping': ['Enhance explosive power and landing.', 'Enhance explosive power and landing.'], 'Walking': ['Focus on smooth strides.']}, 'passes': 13, 'max_speed': 21.103263144234212, 'avg_speed': 14.672951406502698, 'speed_sum': 190.74836828453508, 'speed_count': 13}, '7': {'actions': {'Running': ['Keep steady pace and form.', 'Keep steady pace and form.', 'Keep steady pace and form.', 'Keep steady pace and form.'], 'Jumping': ['Enhance explosive power and landing.', 'Enhance explosive power and landing.'], 'Standing': ['Maintain balance and posture.'], 'Shooting': ['Improve shooting accuracy.'], 'Dribbling': ['Maintain low center of gravity.'], 'Walking': ['Focus on smooth strides.']}, 'passes': 10, 'max_speed': 25.143493037125914, 'avg_speed': 15.665739963592745, 'speed_sum': 156.65739963592745, 'speed_count': 10}, '9': {'actions': {'Shooting': ['Improve shooting accuracy.']}, 'passes': 1, 'max_speed': 23.54166888029226, 'avg_speed': 23.54166888029226, 'speed_sum': 23.54166888029226, 'speed_count': 1}, '8': {'actions': {'Standing': ['Maintain balance and posture.', 'Maintain balance and posture.']}, 'passes': 2, 'max_speed': 13.369948216000155, 'avg_speed': 9.878040462815427, 'speed_sum': 19.756080925630854, 'speed_count': 2}, '1': {'actions': {'Standing': ['Maintain balance and posture.'], 'Walking': ['Focus on smooth strides.']}, 'passes': 2, 'max_speed': 20.833227304339253, 'avg_speed': 20.540133723168267, 'speed_sum': 41.080267446336535, 'speed_count': 2}, '73': {'actions': {}, 'passes': 0, 'max_speed': 0, 'avg_speed': 0, 'speed_sum': 0, 'speed_count': 0}}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tkinter as tka\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "import pytesseract\n",
    "\n",
    "# Initialize Mediapipe pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Set the correct Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "# Initialize drawing utilities for Mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load YOLOv5 model\n",
    "yolo_model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", force_reload=True)\n",
    "\n",
    "# Helper function: Calculate angle\n",
    "def calculate_angle(point1, point2, point3):\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    x3, y3 = point3\n",
    "    angle = np.degrees(np.arctan2(y3 - y2, x3 - x2) - np.arctan2(y1 - y2, x1 - x2))\n",
    "    return abs(angle) if angle >= 0 else abs(angle + 360)\n",
    "\n",
    "# Recognize action\n",
    "def recognize_action(landmarks):\n",
    "    if landmarks is not None:\n",
    "        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        left_wrist = landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "        shoulder_angle = calculate_angle(\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (left_elbow.x, left_elbow.y),\n",
    "            (left_wrist.x, left_wrist.y)\n",
    "        )\n",
    "\n",
    "        if shoulder_angle < 30:\n",
    "            return \"Shooting\", \"Improve shooting accuracy.\"\n",
    "        elif 30 <= shoulder_angle < 45:\n",
    "            return \"Dribbling\", \"Maintain low center of gravity.\"\n",
    "        elif 45 <= shoulder_angle < 75:\n",
    "            return \"Standing\", \"Maintain balance and posture.\"\n",
    "        elif 75 <= shoulder_angle < 105:\n",
    "            return \"Jumping\", \"Enhance explosive power and landing.\"\n",
    "        elif 105 <= shoulder_angle < 130:\n",
    "            return \"Bending\", \"Align knees and back properly.\"\n",
    "        elif 130 <= shoulder_angle < 150:\n",
    "            return \"Walking\", \"Focus on smooth strides.\"\n",
    "        elif 150 <= shoulder_angle <= 180:\n",
    "            return \"Running\", \"Keep steady pace and form.\"\n",
    "    return None, None\n",
    "\n",
    "# Recognize player ID using OCR\n",
    "def recognize_player_id(cropped_frame):\n",
    "    grayscale_frame = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2GRAY)\n",
    "    text = pytesseract.image_to_string(grayscale_frame, config='--psm 8')\n",
    "    player_id = ''.join(filter(str.isdigit, text))\n",
    "    return player_id if player_id else \"Unknown\"\n",
    "\n",
    "# Analyze video\n",
    "def analyze_video(video_path, player_data):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        messagebox.showerror(\"Error\", \"Could not open video file.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = yolo_model(frame)\n",
    "        bbox_list = results.xywh[0].cpu().numpy()\n",
    "\n",
    "        for bbox in bbox_list:\n",
    "            x1, y1, w, h, conf, cls = bbox[:6]\n",
    "            if conf > 0.5:\n",
    "                x1, y1, x2, y2 = int(x1 - w / 2), int(y1 - h / 2), int(x1 + w / 2), int(y1 + h / 2)\n",
    "                cropped_frame = frame[y1:y2, x1:x2]\n",
    "                player_id = recognize_player_id(cropped_frame)\n",
    "                if player_id == \"Unknown\":\n",
    "                    continue\n",
    "\n",
    "                # Initialize player data if not present\n",
    "                if player_id not in player_data:\n",
    "                    player_data[player_id] = {\n",
    "                        \"actions\": {},\n",
    "                        \"passes\": 0,\n",
    "                        \"max_speed\": 0,\n",
    "                        \"avg_speed\": 0,\n",
    "                        \"speed_sum\": 0,\n",
    "                        \"speed_count\": 0,\n",
    "                    }\n",
    "\n",
    "                result = pose.process(cropped_frame)\n",
    "                if result.pose_landmarks:\n",
    "                    action, feedback = recognize_action(result.pose_landmarks)\n",
    "\n",
    "                    if action:\n",
    "                        # Count actions\n",
    "                        player_data[player_id][\"actions\"].setdefault(action, []).append(feedback)\n",
    "\n",
    "                        # Simulate passes and speed calculation\n",
    "                        player_data[player_id][\"passes\"] += 1\n",
    "                        speed = np.random.uniform(1.5, 7.0)  # Simulated speed in m/s\n",
    "                        speed_kmh = speed * 3.6  # Convert to km/h\n",
    "                        player_data[player_id][\"max_speed\"] = max(player_data[player_id][\"max_speed\"], speed_kmh)\n",
    "                        player_data[player_id][\"speed_sum\"] += speed_kmh\n",
    "                        player_data[player_id][\"speed_count\"] += 1\n",
    "\n",
    "    # Calculate average speed\n",
    "    for player in player_data.values():\n",
    "        if player[\"speed_count\"] > 0:\n",
    "            player[\"avg_speed\"] = player[\"speed_sum\"] / player[\"speed_count\"]\n",
    "\n",
    "    print(player_data)\n",
    "    cap.release()\n",
    "\n",
    "# Display stats for players with scroll and color customization\n",
    "def show_stats(player_data, selected_ids):\n",
    "    stats_window = tka.Toplevel()\n",
    "    stats_window.title(\"Player Stats\")\n",
    "    stats_window.geometry(\"800x600\")\n",
    "\n",
    "    # Scrollable frame\n",
    "    canvas = tka.Canvas(stats_window)\n",
    "    scrollbar = tka.Scrollbar(stats_window, orient=\"vertical\", command=canvas.yview)\n",
    "    scrollable_frame = tka.Frame(canvas)\n",
    "\n",
    "    scrollable_frame.bind(\n",
    "        \"<Configure>\", lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\"))\n",
    "    )\n",
    "    canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "    canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "    scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "    # Enable mouse wheel scrolling\n",
    "    canvas.bind_all(\"<MouseWheel>\", lambda event: canvas.yview_scroll(-1 * (event.delta // 120), \"units\"))\n",
    "\n",
    "    tka.Label(scrollable_frame, text=\"Player Stats Analysis\", font=(\"Arial\", 16, \"bold\")).pack(pady=10)\n",
    "\n",
    "    for pid in selected_ids:\n",
    "        if pid not in player_data:\n",
    "            continue\n",
    "\n",
    "        tka.Label(scrollable_frame, text=f\"Player ID: {pid}\", font=(\"Arial\", 14, \"bold\")).pack(pady=5)\n",
    "\n",
    "        # Display max speed and average speed\n",
    "        tka.Label(\n",
    "            scrollable_frame,\n",
    "            text=f\"  Max Speed: {player_data[pid]['max_speed']:.2f} km/h\\n  Avg Speed: {player_data[pid]['avg_speed']:.2f} km/h\",\n",
    "            font=(\"Arial\", 12),\n",
    "        ).pack(pady=5)\n",
    "\n",
    "        # Create table for actions\n",
    "        tree = ttk.Treeview(scrollable_frame, columns=(\"Action\", \"Feedback\", \"Count\"), show=\"headings\", height=5)\n",
    "        tree.heading(\"Action\", text=\"Action\")\n",
    "        tree.heading(\"Feedback\", text=\"Feedback\")\n",
    "        tree.heading(\"Count\", text=\"Count\")\n",
    "        tree.pack(fill=\"x\", pady=5)\n",
    "\n",
    "        actions = player_data[pid][\"actions\"]\n",
    "        for i, (action, feedback_list) in enumerate(actions.items()):\n",
    "            for feedback in set(feedback_list):\n",
    "                tree.insert(\n",
    "                    \"\",\n",
    "                    \"end\",\n",
    "                    values=(action, feedback, feedback_list.count(feedback)),\n",
    "                    tags=(\"oddrow\" if i % 2 == 0 else \"evenrow\",),\n",
    "                )\n",
    "                tree.tag_configure(\"oddrow\", background=\"#f0f8ff\")\n",
    "                tree.tag_configure(\"evenrow\", background=\"#e6e6fa\")\n",
    "\n",
    "# Select Player Stats\n",
    "def select_player(player_data):\n",
    "    selection_window = tka.Toplevel()\n",
    "    selection_window.title(\"Select Player ID\")\n",
    "    selection_window.geometry(\"300x300\")\n",
    "\n",
    "    tka.Label(selection_window, text=\"Select Player IDs (Ctrl+Click for multiple):\").pack(pady=10)\n",
    "\n",
    "    player_listbox = tka.Listbox(selection_window, selectmode=\"multiple\")\n",
    "    player_listbox.pack(fill=\"both\", expand=True, pady=10)\n",
    "\n",
    "    # Add player IDs to the listbox\n",
    "    for player_id in player_data.keys():\n",
    "        player_listbox.insert(\"end\", player_id)\n",
    "\n",
    "    def view_selected_stats():\n",
    "        selected_indices = player_listbox.curselection()\n",
    "        selected_ids = [player_listbox.get(i) for i in selected_indices]\n",
    "        if not selected_ids:\n",
    "            messagebox.showinfo(\"Info\", \"No player selected.\")\n",
    "            return\n",
    "        show_stats(player_data, selected_ids)\n",
    "        selection_window.destroy()\n",
    "\n",
    "    tka.Button(selection_window, text=\"View Stats\", command=view_selected_stats).pack(pady=10)\n",
    "\n",
    "# Tkinter UI\n",
    "def main():\n",
    "    player_data = {}\n",
    "\n",
    "    def upload_video():\n",
    "        video_path = filedialog.askopenfilename(\n",
    "            title=\"Select a video\", filetypes=[(\"MP4 files\", \"*.mp4\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        if video_path:\n",
    "            analyze_video(video_path, player_data)\n",
    "            messagebox.showinfo(\"Info\", \"Video analysis completed!\")\n",
    "\n",
    "    root = tka.Tk()\n",
    "    root.title(\"Sports Performance Analysis\")\n",
    "\n",
    "    tka.Label(root, text=\"Sports Performance Analysis\", font=(\"Arial\", 18, \"bold\")).pack(pady=20)\n",
    "    tka.Button(root, text=\"Upload Video\", command=upload_video, font=(\"Arial\", 14)).pack(pady=10)\n",
    "    tka.Button(root, text=\"Select Player\", command=lambda: select_player(player_data), font=(\"Arial\", 14)).pack(pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
