{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 1:  Video Upload and Frame Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Directory for video uplaod and frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame extraction completed. Frames are saved in your_video_frames\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to create directory for storing frames\n",
    "def create_frame_dir(video_file):\n",
    "    frame_dir = os.path.splitext(video_file)[0] + \"_frames\"\n",
    "    if not os.path.exists(frame_dir):\n",
    "        os.makedirs(frame_dir)\n",
    "    return frame_dir\n",
    "\n",
    "# Function to extract and save frames from the video\n",
    "def extract_frames(video_file):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    # Create directory to store the frames\n",
    "    frame_dir = create_frame_dir(video_file)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # If no frame is captured, exit\n",
    "        \n",
    "        # Save frame as image\n",
    "        frame_path = os.path.join(frame_dir, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        print(f\"Extracted frame {frame_count}\")\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Frame extraction completed. Frames are saved in {frame_dir}\")\n",
    "\n",
    "# Call the function with your video file path\n",
    "video_file = \"your_video.mp4\"  # Replace with your video file\n",
    "extract_frames(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Player Detection Using YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use YOLOv5 to detect players in frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dhira/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-5 Python-3.12.3 torch-2.4.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load YOLOv5 pre-trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Detect players in the extracted frames\n",
    "def detect_players_in_frames(frame_dir):\n",
    "    frame_paths = list(Path(frame_dir).glob('*.jpg'))\n",
    "    \n",
    "    for frame_path in frame_paths:\n",
    "        # Perform detection\n",
    "        results = model(frame_path)\n",
    "        \n",
    "        # Display results (bounding boxes around detected players)\n",
    "        results.show()  # This will show the image with detection\n",
    "\n",
    "        # If you want to save the results, uncomment the following line\n",
    "        # results.save()  # This will save images with bounding boxes to the current directory\n",
    "        \n",
    "        print(f\"Detected players in {frame_path}\")\n",
    "\n",
    "# Call the function with the frame directory\n",
    "frame_dir = \"your_video_frames\"  # Replace with your frame directory\n",
    "detect_players_in_frames(frame_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tracking the Player Across Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.0\n",
      "scikit-learn version: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Player tracking with DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the DeepSORT object\n",
    "deepsort = DeepSort('ckpt.t7')  # You need the pre-trained checkpoint model\n",
    "\n",
    "# Function to track players across frames\n",
    "def track_players_in_video(video_file, frame_dir):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit if no frame is captured\n",
    "\n",
    "        # Perform detection (similar to part 2)\n",
    "        results = model(frame)\n",
    "        bboxes = results.xyxy[0][:, :4].cpu().numpy()  # Bounding boxes\n",
    "        confs = results.xyxy[0][:, 4].cpu().numpy()    # Confidence scores\n",
    "\n",
    "        # Perform tracking\n",
    "        outputs = deepsort.update(bboxes, confs, frame)\n",
    "\n",
    "        # Draw the bounding boxes and track IDs on the frame\n",
    "        for output in outputs:\n",
    "            x1, y1, x2, y2, track_id = output[:5]\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"ID: {track_id}\", (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Tracking\", frame)\n",
    "        frame_count += 1\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the tracking function\n",
    "track_players_in_video(video_file, frame_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Pose Estimation Using OpenPose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Implement Pose Estimation on Extracted Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize MediaPipe Pose module\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "# Function to perform pose estimation on a single frame\n",
    "def pose_estimation_on_frame(frame):\n",
    "    # Convert frame to RGB as MediaPipe works with RGB images\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform pose estimation\n",
    "    results = pose.process(rgb_frame)\n",
    "\n",
    "    # Draw keypoints on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    if results.pose_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(annotated_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    return annotated_frame, results.pose_landmarks\n",
    "\n",
    "# Process all frames for pose estimation\n",
    "def process_pose_estimation_on_video(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the pose estimation function with your video\n",
    "video_file = 'path_to_your_video.mp4'  # Specify the path to your video\n",
    "process_pose_estimation_on_video(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define Actions and Recognition Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Action Recognition\n",
    "\n",
    "def recognize_action(landmarks):\n",
    "    if landmarks is not None:\n",
    "        # Example: Get key points\n",
    "        left_shoulder = landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "        left_elbow = landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW]\n",
    "        left_wrist = landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "        # Define simple thresholds for action recognition\n",
    "        shoulder_angle = calculate_angle(\n",
    "            (left_shoulder.x, left_shoulder.y),\n",
    "            (left_elbow.x, left_elbow.y),\n",
    "            (left_wrist.x, left_wrist.y)\n",
    "        )\n",
    "\n",
    "        if shoulder_angle < 45:  # This is just an example threshold\n",
    "            return \"Action: Shooting\"\n",
    "        elif shoulder_angle > 135:\n",
    "            return \"Action: Running\"\n",
    "    \n",
    "    return \"Action: Unknown\"\n",
    "\n",
    "# Modify the video processing function to include action recognition\n",
    "def process_pose_estimation_on_video_with_action_recognition(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the action recognition function with your video\n",
    "process_pose_estimation_on_video_with_action_recognition(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Generating Feedback for Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Generate Feedback Based on Recognized Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: Generating Feedback for Players\n",
    "\n",
    "def generate_feedback(action):\n",
    "    feedback = \"\"\n",
    "    \n",
    "    if action == \"Action: Shooting\":\n",
    "        feedback = \"Feedback: Focus on improving your shooting technique. Aim for accuracy.\"\n",
    "    elif action == \"Action: Running\":\n",
    "        feedback = \"Feedback: Maintain a steady pace and good form while running.\"\n",
    "    else:\n",
    "        feedback = \"Feedback: Keep practicing your actions for better performance.\"\n",
    "    \n",
    "    return feedback\n",
    "\n",
    "# Modify the video processing function to include feedback generation\n",
    "def process_pose_estimation_on_video_with_feedback(video_file):\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform pose estimation\n",
    "        annotated_frame, landmarks = pose_estimation_on_frame(frame)\n",
    "\n",
    "        # Recognize action\n",
    "        action = recognize_action(landmarks)\n",
    "        print(action)\n",
    "\n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(action)\n",
    "        print(feedback)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow(\"Pose Estimation\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the feedback generation function with your video\n",
    "process_pose_estimation_on_video_with_feedback(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Simple Implementation of getting video and anlysis them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback for video1.mp4: Analysis completed for video1.mp4\n",
      "Feedback for video2.mp4: Analysis completed for video2.mp4\n",
      "Feedback for video3.mp4: Analysis completed for video3.mp4\n"
     ]
    }
   ],
   "source": [
    "def analyze_video(video_path):\n",
    "    # Placeholder for video analysis logic\n",
    "    # Replace this with actual video analysis code\n",
    "    # For now, let's just return a dummy feedback\n",
    "    return f\"Analysis completed for {video_path}\"\n",
    "\n",
    "def test_videos(video_list):\n",
    "    results = []\n",
    "    for video_path in video_list:\n",
    "        feedback = analyze_video(video_path)  # Call the video analysis function\n",
    "        results.append((video_path, feedback))\n",
    "    return results\n",
    "\n",
    "# Sample usage\n",
    "video_list = ['video1.mp4', 'video2.mp4', 'video3.mp4']\n",
    "results = test_videos(video_list)\n",
    "for video, feedback in results:\n",
    "    print(f\"Feedback for {video}: {feedback}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_threshold = 0.5  # Initial threshold\n",
    "\n",
    "def set_action_threshold(new_threshold):\n",
    "    global action_threshold\n",
    "    action_threshold = new_threshold\n",
    "\n",
    "# Example of updating the threshold\n",
    "set_action_threshold(0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def log_feedback(video_path, actions, feedback):\n",
    "    with open('feedback_log.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([video_path, actions, feedback])\n",
    "\n",
    "# Sample logging\n",
    "log_feedback('video1.mp4', 'Running, Passing', 'Improve passing accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def initialize_db():\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS performance (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            video_path TEXT,\n",
    "            actions TEXT,\n",
    "            feedback TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_performance(video_path, actions, feedback):\n",
    "    conn = sqlite3.connect('player_analysis.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        INSERT INTO performance (video_path, actions, feedback) \n",
    "        VALUES (?, ?, ?)\n",
    "    ''', (video_path, actions, feedback))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Initialize the database\n",
    "initialize_db()\n",
    "\n",
    "# Insert sample performance\n",
    "insert_performance('video1.mp4', 'Running, Passing', 'Improve passing accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def upload_video():\n",
    "    video_path = filedialog.askopenfilename()\n",
    "    if video_path:\n",
    "        feedback = analyze_video(video_path)  # Replace with your video analysis function\n",
    "        messagebox.showinfo(\"Feedback\", feedback)\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Player Performance Analysis\")\n",
    "\n",
    "upload_button = tk.Button(root, text=\"Upload Video\", command=upload_video)\n",
    "upload_button.pack(pady=20)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request, render_template\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route('/')\n",
    "# def home():\n",
    "#     return render_template('index.html')\n",
    "\n",
    "# @app.route('/upload', methods=['POST'])\n",
    "# def upload():\n",
    "#     if 'video' not in request.files:\n",
    "#         return \"No file uploaded\", 400\n",
    "#     video = request.files['video']\n",
    "#     video_path = f'uploads/{video.filename}'\n",
    "#     video.save(video_path)\n",
    "#     feedback = analyze_video(video_path)  # Replace with your analysis function\n",
    "#     return feedback\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30752</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,968,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30752\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m1,968,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,969,738</span> (7.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,969,738\u001b[0m (7.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,969,738</span> (7.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,969,738\u001b[0m (7.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Sample usage\n",
    "input_shape = (64, 64, 3)  # Example input shape\n",
    "num_classes = 10  # Set the number of classes (e.g., for 10 classes)\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Print model summary to verify the architecture\n",
    "model.summary()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
